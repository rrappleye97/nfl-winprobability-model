# Methodology

Instead of predicting win probability from any game state with one full-stop model, win probability is predicted only from kickoff game states^[Each of the models will be described in greater detail later on in the section]. When the game state is not a kickoff, a series of models first specifies probability distributions for the more granular events that eventually lead to a kickoff and samples are taken from these probability distributions to create a distribution of the possible future game states at the next kickoff. Win probability is then taken to be the mean of the win probability of the distribution of sampled future game states. To implement this a given play is sorted into one of four possible game states: kickoff, extra point, first and ten, and all other game states.   

## Modeling Win Probability on Kickoffs and Point After Attempts

**Model when more than ten minutes remain:**  
On kickoffs a win probability is modeled using a variation of the trick PFR uses to implement a normal distribution when deriving win probability for a given game state $i$:  

$\Delta score_i \sim \mathcal{N}(\mu_i,\sigma_i^{2})$ where  

$\hat{\mu}_i = \beta_{int} + \beta_\rho\rho_i + \beta_{pos_o}{pos_o}_i + \beta_{de\hspace{-0.1em}f_o}{de\hspace{-0.1em}f_o}_i + \beta_{s_{curr}}s_{curr_i}\:+$  
$\beta_kk_i + \beta_{kpos_o}k_ipos_{o_i} + \beta_{k{de\hspace{-0.1em}f_o}}k_i{de\hspace{-0.1em}f_{o_i}} + \beta_{s_{curr}pos_o}s_{curr_i}pos_{o_i} + \beta_{s_{curr}{de\hspace{-0.1em}f_o}}s_{curr_i}{de\hspace{-0.1em}f_{o_i}}$ 

and  

$\hat{\sigma}_i = \beta_{int} + \beta_{pos_o}{pos_o}_i + \beta_{de\hspace{-0.1em}f_o}{de\hspace{-0.1em}f_o}_i + \beta_{pos_d}{pos_d}_i + \beta_{de\hspace{-0.1em}f_d}{de\hspace{-0.1em}f_d}_i  + \beta_{s_{curr}}s_{curr_i} + \beta_{s_{curr_{abs}}}\mid\hspace{-0.2em}{s_{curr_i}}\hspace{-0.2em}\mid + \beta_tt_i \: +$  
$\beta_{ts}\sqrt{t_i} + \beta_{pos_{qb}}{pos_{qb}}_i + \beta_{de\hspace{-0.1em}f_{qb}}{de\hspace{-0.1em}f_{qb}}_i$ + $\beta_{pos_{qb}s}{pos_{qb}}_is_i + \beta_{de\hspace{-0.1em}f_{qb}s}{de\hspace{-0.1em}f_{qb}}_is_i$

Where we define:  
$int =$ Intercept,  
$\rho =$ = Scaled point spread  
$s_{curr} =$ Current score differential  
$k =$ Team that receives the second half kickoff^[1 if the team that has the ball will receive the second half kickoff, -1 if the defensive team will receive the second half kickoff, and 0 if the game is already past halftime]  
$t =$ Time remaining in seconds  
$pos_o =$ Offensive DVOA for the team possessing the football   
$pos_d =$ Defensive DVOA for the team possessing the football   
$de\hspace{-0.1em}f_o =$ Offensive DVOA for the team on defense  
$de\hspace{-0.1em}f_d =$ Defensive DVOA for the team on defense  
$pos_{qb} =$ Projected PFF grade for the quarterback of the team with the ball  
$de\hspace{-0.1em}f_{qb} =$ Projected PFF grade for the quarterback of the team on defense  

By making the observation that the final score differential is simply $s_{curr} + \Delta score$ we find:

<center>

$s_{final_i} \sim \mathcal{N}(\mu_i + s_{curr_i},\,\sigma_i^{2})$ and therefore:
$WinProb_i = P(s_{final_i} > 0) = CDF^{-1}(\frac{\mu_i + s_{curr_i}}{\sigma_i})$

</center>

Where we define:  
$s_{final} =$ Final score differential

The predictors for $\mu$ were chosen by performing cross validation in which the most recent season in the training set, 2016, was left out and used as a test set^[The most recent season was chosen as the season to leave out because the model will always be tasked with using past results to predict future results in practice, and it seems likely that rule changes and evolving strategy could cause gradual changes in game play over time. If these changes are even somewhat smooth, it will be useful to tune models on data that will differ from the training set in a similar manner to the way the next season’s batch of data will differ from the current season’s.]. Horowitz, Ventura, and Yurko (2018) used a similar method and called it “leave-one-season-out-cross-validation (LOSO CV)”, a term I will modify to leave-most-recent-season-out-cross-validation and brand LMRSO CV. Brier score and log-loss were the primary estimates used to evaluate model performance. The simple linear model used was also tested against a mixed linear model that treated $k$—receiving the second half kickoff—as a random effect and utilized an additional intercept as well as random slopes for $pos_o$ and $de\hspace{-0.1em}f_o$, but it performed no better than the simple linear model.

Predictors for sigma were also chosen using LMRSO CV. The linear model used for sigma was also tested against GAMs that were fit with Gamma and Log-Normal distributions using the “gamlss” package developed by Rigby and Stasinopoulos (2005). The “gamlss” package was also used to fit a GAM with an InverseGamma distribution to model $\sigma^{2}$, but the linear model performed the best by mean-squared error when using LMRSO CV^[Error for $\sigma^{2}$ was determined after taking the square root]. This was surprising because the distributions specified when fitting the GAMs seemed more appropriate given the distribution of $\sigma$ and $\sigma^{2}$. Ultimately, however, it made the most sense to use the model with the best estimate of $\sigma$, because only a point estimate of $\sigma$ is taken as a parameter of the normal distribution.

A Generalized Linear Mixed Model (GLMM) fit using the “lme4” package and a Generalized Boosted Model (GBM) fitted using Greg Ridgeway’s “gbm” package, used to extend Jerome Friedman’s Gradient Boosted Machine (2001 & 2002). The above model outperforms both the GLMM and the GBM.

**Model when ten or fewer minutes remain**  
The assumption that $\Delta score$ is normally distributed begins to break down some time around the ten minute mark of the fourth quarter. Since it is no longer valid to model $\Delta score$ with a normal distribution, win probability is instead modeled using a GBM fit with the following terms using the “gbm” package.

```{r win-prob-vars, results="asis", echo=FALSE, cache = TRUE}
win.prob.4th.vars = c("Scaled Point Spread", "Current Score Differential", "Square Root of Adjusted Time Left in the Half", "Offensive DVOA for the Offensive Team", "Offensive DVOA for the Defensive Team", "Defensive DVOA for the Offensive Team", "Defensive DVOA for the Defensive Team", "Projected QB grade for the Offensive Team", "Projected QB grade for the Offensive Team", "Timeouts left for the Offense", "Timeouts left for the Defense")
win.prob.4th.vars = as.data.frame(win.prob.4th.vars)
colnames(win.prob.4th.vars) = c("Variables")
kable(win.prob.4th.vars, 
      caption = "Variables Used in the 4th Quarter Win Probability GBM",
      longtable = TRUE,
      booktabs = TRUE)
```

Both a Random Forest and a GBM were considered because of their penchant for fitting data that features predictors with ambiguous interactions, but a GBM was chosen because it performed better in LMRSO CV. Friedman’s Gradient Boosted Machine algorithm (2001 & 2002), implemented in the “gbm” package, works by additively growing a forest of $n$ decision trees where each tree has a maximum depth $d$.Trees are added to the forest using gradient descent—each tree must reduce the residual loss as defined by Friedman's $K$-class loss function (2001):

<center>

$-\sum_{k=1}^Ky_klog(p_k(x))$

</center>

A shrinkage parameter $l$ exists to reduce the learning rate of trees. Lower values generally protect against overfitting but often require more trees to maximize predictive power.

**Overtime model**  
A GBM fit with the same variables as above but trained only on overtime data is used to determine win probability in overtime situations. 

**Extra Points**  
For extra points the process only involves the additional step of sampling $n$ outcomes for the extra point or two point conversion try^[A chart defining situations where a team is classified as going for two is listed in the appendix] and passing each resulting game state $i*$ to the model that predicts win probability to find:

$WinProb_i = \frac{1}{n} \sum_{k=1}^nWinProb_{i*}$, where $WinProb_i*$ =    

  $CDF^{-1}(\frac{\mu_{i*} + s_{curr_{i*}}}{\sigma^{2}_{i*}})$ if $t_{i*} > 600$,  
  $GBM_{4th}(i*)$ if $0 <t_{i*} \leq 600$, and   
  $GBM_{ot}(i*)$ if $t_{i*} < 0$  

## Modeling Win Probability on First and Ten
When the game state features a first and ten we must take $n$ samples of the pair (next scoring event, time elapsed) where, just as in Horowitz, Ventura, and Yurko (2018) the seven possible outcomes for the next scoring event include a touchdown, field goal, or safety for either team and no scoring events before the end of the half. As is done by Horowitz, Ventura and Yurko (2018), the next scoring event is modeled by a multinomial logistic regression, though this work uses the Brian Ripley’s “nnet” package to implement a multinomial logistic regression instead of comparing a set of binary logistic regressions with a chosen baseline event. Modeling end of half, end of game, and overtime situations separately gave the most robust performance by LMRSO CV. The formulas for some of the linear models are the same, but each model was trained only on data within the time window where it would be applied. A multinomial GBM model was also considered. For a given initial game state $i$ we find the log odds for each possible outcome $x$ of being the next scoring event $\delta_x$ as compared to the outcome the algorithm chooses as the baseline^[A conversion in this case]:

$\delta_{ix} = \beta_{intx} + \beta_{xpos_o}{pos_o}_{i} + \beta_{xde\hspace{-0.1em}f_o}{de\hspace{-0.1em}f_o}_i + \beta_{xpos_d}{pos_d}_i + \beta_{xde\hspace{-0.1em}f_d}{de\hspace{-0.1em}f_d}_i + \beta_{xpos_{st}}{pos_{st}}_i + \:$  
$\beta_{xde\hspace{-0.1em}f_{st}}{de\hspace{-0.1em}f_{st}}_i + \beta_{xt_{adjH}}t_{adjH_i} + \beta_{xc_{score}}c_{score_i} + \beta_{xyrd}yrd_i$ if $t_i > 300 \:\: \& \:\: 1800 < t_i \leq 1980$,  

$\delta_{ix} = \beta_{xint} + \beta_{xpos_o}{pos_o}_i + \beta_{xde\hspace{-0.1em}f_o}{de\hspace{-0.1em}f_o}_i + \beta_{xpos_d}{pos_d}_i + \beta_{xde\hspace{-0.1em}f_d}{de\hspace{-0.1em}f_d}_i + \beta_{xpos_{st}}{pos_{st}}_i + \:$  
$\beta_{xde\hspace{-0.1em}f_{st}}{de\hspace{-0.1em}f_{st}}_i + \beta_{xt_{adjH}}t_{adjH_i} + \beta_{xc_{score}}c_{score_i} + \beta_{xyrd}yrd_i$ if $0 < t_i \leq 300$,  

$\delta_{ix} = \beta_{xint} + \beta_{xpos_o}{pos_o}_i + \beta_{xde\hspace{-0.1em}f_o}{de\hspace{-0.1em}f_o}_i + \beta_{xpos_d}{pos_d}_i + \beta_{xde\hspace{-0.1em}f_d}{de\hspace{-0.1em}f_d}_i + \beta_{xpos_{st}}{pos_{st}}_i + \:$  
$\beta_{xde\hspace{-0.1em}f_{st}}{de\hspace{-0.1em}f_{st}}_i + \beta_{xt_{adjH}}t_{adjH_i} + \beta_{xyrd}yrd_i$ if $1800 < t_i \leq 1980$, and  

$\delta_{ix} = \beta_{xint} + \beta_{xpos_o}{pos_o}_i + \beta_{xde\hspace{-0.1em}f_o}{de\hspace{-0.1em}f_o}_i + \beta_{xpos_d}{pos_d}_i + \beta_{xde\hspace{-0.1em}f_d}{de\hspace{-0.1em}f_d}_i + \beta_{xpos_{st}}{pos_{st}}_i + \:$  
$\beta_{xde\hspace{-0.1em}f_{st}}{de\hspace{-0.1em}f_{st}}_i + \beta_{xt_{adjH}}t_{adjH_i} + \beta_{xyrd}yrd_i$ if $t_i \leq 0$ 

Where we define:   
${pos_{st}} =$ Special teams DVOA for the team with the ball  
$de\hspace{-0.1em}f_{st} =$ Special teams DVOA for the defensive team  
$c_{score} =$ Comeback score^[The comeback score variable is meant to define the urgency of a comeback situation to help the models that measure time elapsed provide more accurate probability distributions when the game state is being advanced. It is also used in some of the end of game models as a feature that gives an indication of the relationship between score differential and time remaining.]  
$yrd =$ Number of yards needed for an offensive touchdown  
$t_{adjH} =$ Timeout adjusted time^[The amount of time remaining in the half + 20 secs for every timeout possessed by a team with a comeback score of four or five] remaining in the half  

We don't need to do any of the work converting the pairwise log odds into a distribution as the "nnet" package does this. We will let $\delta_i$ denote the multinomial distribution of scoring events that we will be sampling from move right and draw $n$ samples $nextScore_{i*_1}, \: nextScore_{i*_2},\: ..., \: nextScore_{i*_n} \sim \mathcal{Multinomial}(\delta_i)$. For samples where the next scoring play is a touchdown, the game state is updated to include a draw for the point after that uses the same method outlined in the "Extra Points" sub-heading of 2.1.

Sampling the time elapsed until the next scoring event is a bit trickier^[Except when the next scoring play is sample to be the end of the half in which case time elapsed is just the time remaining in the half]. First, a linear model^[It outperformed a GAM assuming a Log-Normal distribution with the same inputs by LMRSO CV] predicts the number of plays, $plays_{i*}$ for each sample $i*$ that take place from the initial game state $i$ until kickoff for each first and ten game state:  

$\hat{plays_{i*}} = \beta_{int} + \beta_{pos_o}{pos_o}_i + \beta_{de\hspace{-0.1em}f_o}{de\hspace{-0.1em}f_o}_i + \beta_{pos_d}{pos_d}_i + \beta_{de\hspace{-0.1em}f_d}{de\hspace{-0.1em}f_d}_i + \beta_{yrd}\sqrt{yrd_i} + \beta_{c_{score}}c_{score_i} + \:$
$\beta_{t_{adjH}}\sqrt{t_{adjH_i}} + \beta_{nextScore}nextScore_{i*} + \beta_{c_{score}yrd}c_{score_i}\sqrt{yrd_i}$

The distributions of the number of plays until the next scoring plays seems to more closely follow a Log-Normal distribution than a Normal distribution:

```{r test-log-norm, results="asis", echo=FALSE, cache = TRUE}
data = readRDS("/home/guest/thesis-win-probability-model/cleaned-data-part-4.Rda")
data = data[which(data$year < 2016 & data$kickoff==1 & !is.na(data$n.plays.until.kickoff)),]
ggplot() +
  geom_density(aes(x = data$n.plays.until.kickoff), color = "steelblue") +
  labs(x = "Number of plays", y = "Density", title = "Density Plot of the Number of Plays from One Score to the Next")
```

To allow for more accurate sampling of the number of plays from the current game state to the next scoring play the "fitdistrplus” package by Marie-Laure Delignette-Muller is used to choose the parameters of a Log-Normal distribution that fit the data best. $\sigma^{2}_{plays}$ is set to the variance parameter of the chosen Log-Normal and we sample:

<center>

$log(plays_{i*}) \sim \mathcal{N}(log(\hat{plays_{i*}}),\sigma_{plays}^{2})$ so  
$plays_{i*} = e^{log(plays_{i*})}$  

</center>

To transform each sampled number of plays to time space we split the plays into plays that stopped the clock and plays where the clock continued to run. 

<center>

$nStopped_{i*} \sim \mathcal{Binom}(plays_{i*}, p_{stopped_{i*}})$ and  
$nRunning_{i*} = plays_{i*} - nStopped_{i*}$

</center>

Where $p_{stopped_{i*}}$ is derived from a GBM detailed in the appendix. Draws that result in fewer than one clock stop are rejected because every scoring play stops the clock.  

We next find the total time elapsed. We start by letting  
$K = nStopped_{i*}$,  
$J = nRunning_{i*}$,  
$S =$ time elapsed for a play after which the clock stops  
$R =$ time elapsed for a play after which the clock runs:  

  $S_{i*_{1}}, \: S_{i*_{2}}, \: ...  , \: S_{i*_{K}} \sim \mathcal{N}(\hat{S_i},\sigma_{S}^{2})$ where
  $\hat{S_i} = \beta_{int} + \beta_{down}down_i + \beta_{c_{score}}c_{score_i} + \beta_{ydstogo}ydstogo_i + \beta_{c_{score}down}c_{score_i}down_i$  
  
  and  

  $R_{i*_{1}}, \: R_{i*_{2}}, \: ...  , \: R_{i*_{J}} \sim \mathcal{N}(\hat{R_i},\sigma_{R}^{2})$ where
  $\hat{R_i} = \beta_{int} + \beta_{down}down_i + \beta_{c_{score}}c_{score_i} + \beta_{ydstogo}ydstogo_i + \beta_{c_{score}down}c_{score_i}down_i$ 

We then find the time elapsed until the next scoring play as:  
$elapsed_{i*} = \sum_{k=1}^{K}S_k + \sum_{j=1}^{J}R_j$


Samples from ${N}(\mu_{t_{stop_i}},\sigma_{t_{stop}}^{2})$ are rejected if they fall outside $[0, 15]$ and samples from ${N}(\mu_{t_{run_i}},\sigma_{t_{run}}^{2})$ are rejected if they fall outside $[15, 50]$ in order to restrict sampled play lengths to a realistic distribution. Additionally, the entire process for sampling a particular value for time elapsed until the next scoring event is repeated if the sample causes an invalid game state^[If a sample is rejected more than 10 times it is that the game state that will result in the end of the half and was misclassified] (i.e. the time elapsed is greater than the amount of time remaining in the half).


Finally, the game states $i*_1, \:i*_2, \:..., \: i*_n \sim I^*$ are updated in accord with the values that have been sampled and we have:  

$WinProb_i = \frac{1}{n} \sum_{i*}^{I^*}(WinProb_{i*})$, where $WinProb_{i*}$ = 

  $CDF^{-1}(\frac{\hat{\mu}_{i*} + s_{curr_{i*}}}{\sigma^{2}_{i*}})$ for $t_{i*} > 600$,   
  $GBM_{4th}(i*)$ for $t_{i*}$ such that $0 < t_{i*} \leq 600$,   
  $GBM_{ot}(i*)$ for $t_{i*} < 0$   

## Modeling Win Probability in Other Scenarios

When the game state does not fit any of the scenarios above^[This will occur on any second, third, or fourth down play, as well as on first downs where there are more or less than ten yards to go], it is first advanced to the next set of downs. To do this we take m samples of the triple (outcome of the current set of downs, time elapsed until the next set of downs, current set of downs results in scoring play). We also sample the number of yards downfield the ball will move over the current set of downs for all draws where the current set of downs does not result in a scoring play. 

Seven possible outcomes are also defined for a given set of downs: conversion, turnover on downs, end half, field goal attempt, punt, safety, and turnover. As was the case when modeling the next scoring event, end of half, end of game, and overtime game states are modeled separately from other game states. Additionally, separate models were fit for each down because the log loss was lower for each down when separate models were fitted for each during LMRSO CV. End of half, end of game and overtime game states were all modeled with separate GBMs fit using the “gbm” package while all other game states were modeled with a multinomial logistic regression fit with the “nnet” package. The model formulas are listed in the appendix. The models all utilize down, yards to go, DVOA ratings, quarterback grades and a version of yard line that treats each 10 yard increment of the field as a separate group to help the model make better decisions about the likelihood of field goal attempts, punts, and fourth down conversion attempts. 


```{r fg-by-yardline, results="asis", echo=FALSE, cache = TRUE}
data = readRDS("/home/guest/thesis-win-probability-model/cleaned-data-part-4.Rda")
data = data[which(data$year < 2016),]
conv.prob = rep(NA, 20)
downs.prob = rep(NA, 20)
punt.prob = rep(NA, 20)
turnover.prob = rep(NA, 20)
fg.prob = rep(NA, 20)
safety.prob = rep(NA, 20)
end.half.prob = rep(NA, 20)
for(i in c(1:20)){
  tab = table(data$downs.outcome[which(data$yrdline100 > 5*(i-1) & data$yrdline100 <= 5*i)])
  conv.prob[i] = tab[which(names(tab) == "Converted")]/sum(tab)
  fg.prob[i] = ifelse(length(which(names(tab) == "Field Goal")) > 0, tab[which(names(tab) == "Field Goal")]/sum(tab), 0)
  safety.prob[i] = ifelse(length(which(names(tab) == "Safety")) > 0, tab[which(names(tab) == "Safety")]/sum(tab), 0)
  punt.prob[i] = ifelse(length(which(names(tab) == "Punt")) > 0, tab[which(names(tab) == "Punt")]/sum(tab), 0)
  turnover.prob[i] = tab[which(names(tab) == "Turnover")]/sum(tab)
  downs.prob[i] = ifelse(length(which(names(tab) == "Downs")) > 0, tab[which(names(tab) == "Downs")]/sum(tab), 0)
  end.half.prob[i] = tab[which(names(tab) == "End Half")]/sum(tab)
}

outcome = c(rep("Conversion", 20), rep("Downs", 20), rep("End Half", 20), rep("Field Goal", 20), rep("Punt", 20), rep("Safety", 20), rep("Turnover", 20))
probs = c(conv.prob, downs.prob, end.half.prob, fg.prob, punt.prob, safety.prob, turnover.prob)

ggplot() +
  geom_line(aes(x = rep(c(1:20),7), y = probs, color = outcome)) +
  labs(x = "Yards from Endzone", y = "Probability of Being Outcome", title = "P(Downs Outcome) by Yards from Endzone", subtitle = "2010 to 2015 NFL data")

```

After we have created a distribution $\delta_i$ of potential outcomes of the set of downs associated with our inital game state $i$ and sampled $n$ outcomes $outcome_{i*_1}, \: outcome_{i*_2}, \:..., \:outcome_{i*_n} \sim \mathcal{Multinomial}(\delta_i)$, we move on to sampling from the distribution of time elapsed. We begin by sampling the number of plays until the next set of downs. This is treated as:

$Plays_{i*} = downstogo_{i*} + replaysByPenalty_{i*}$ where
$downstogo_{i*} = 5 - down_i$ for punts, turnovers on downs, and field goal attempts and  
$downstogo_{i*} \sim \mathcal{Multinomial}(outcomeDist)$ for conversions, safeties, and turnovers where  
$outcomeDist =$    
  $(0.349, 0.348, 0.276, 0.027)$ for conversions,  
  $(0.363, 0.312, 0.293, 0.032)$ for turnovers, and  
  $(0.333, 0.228, 0.272, 0.167)$ for safeties. Additionally,  
$replaysByPenalty_{g_k} \sim \mathcal{Binom}(downstogo_{g_k}, p_{penalty_i})$ where  
$p_{penalty_i}$ is taken from a GBM that predicts the probability of a given play being replayed due to penalty. It is listed in the appendix.   

$downstogo_{g_k}$ is set to be $5 -  down_{curr_i}$ when $outcome_{g_k}$ is a punt, turnover on downs or field goal attempt because those plays are assumed to happen only on fourth down. The multinomial distributions for conversions, turnovers and safeties simply refer to the probability that an outcome of the given type for a set of downs occurs on a given down. Samples from the distributions that imply the event happened on a down that has already occurred are rejected. Penalty replays refers to the number of plays until the next set of downs that will be replayed by penalty. To translate a sampled number of plays into time space we use the same equations and process that was detailed in section 2.2.

We next sample whether a given set of downs will result in a scoring play^[All sets of downs where the sampled outcome is a safety are treated as scoring plays]. 

$score_{g_k} \sim \mathcal{Multinomial}(p_{fg_i}, p_{defTD_i}, p_{noScore_i})$ if $outcome_{g_k} =$ fg attempt
$score_{g_k} \sim \mathcal{Binomial}(p_{score_i} | \:outcome_{g_k})$ else

$p_{score_i} | \:outcome_{g_k}$ is predicted by a GBM for sampled outcomes that are able to result in scores—conversions, field goals, punts and turnovers^[Scores on conversions are assumed to be offensive touchdowns and scores on punts and turnovers are assumed to be defensive touchdowns]. Each GBM was fit using every set of downs that ended with the given outcome, and is listed in the appendix. For samples where score = 1 and the score is a touchdown, the game state is updated to include a draw for the point after that uses the same method outlined in the extra points sub-heading of 2.1. For these samples, as well as any samples where the $outcome_{g_k}$ is the end of the half, we find that for each updated game_state $g_{k}$ the win probability is calculated in the same manner as it was in 2.2:

$WinProb_{g_k}$ is:  
  $CDF^{-1}(\frac{\mu_{g_k} + s_{curr_{g_k}}}{\sigma_{g_k}})$ for $t_{g_k} > 600$,   
  $GBM_{4th}(g_k)$ for $t_{g_k}$ such that $0 < t_{g_k} \leq 600$,   
  $GBM_{ot}(g_k)$ for $t_{g_k} < 0$   

For samples where score = 0 we must also sample a value for the number of yards downfield the ball will move over the current set of downs before calculating win probability. For each outcome other than conversions and field goal attempts^[and end of halves where we don't need to sample this quantity at all], two linear models are defined—one for downs first through third and one for fourth down—that specify a distribution for the number of yards downfield the ball will move for a given game state. Each linear model is listed in the appendix. The yards moved downfield is modeled separately for each possible outcome because doing so helped most of the linear models meet the assumption of constant residual variance and normally distributed errors^[The exception to this is when the outcome of the set of downs is a conversion, but rejecting samples that were behind the first down marker alleviated this.], allowing for easier sampling. The same reasoning is behind separating fourth down data from other data when fitting each model—the residual variance should be lower on fourth down for each outcome type because there is no longer uncertainty about where a punt, field goal attempt, etc. will be taken from. Conversions are a special case and are split into regular conversions and conversions by penalty.

<center>
$ConversionType_{g_k} \sim \mathcal Bernoulli(p_{penaltyConv_i})$ if $outcome_{g_k} =$ conversion  
</center>  


$p_{penaltyConv_i}$ is defined by a logistic regression that has been trained on all conversion plays. The model only looked at the current down, the yards to go for a first and the interaction between the two variables. Yards moved downfield for conversions by penalty are also modeled with two linear models, one for fourth down and one for any other down. For regular conversions a linear model is specified for each down to better satisfy the linear regression constraints. Field goal attempts also work differently. Field goal attempts that don’t result in a score are classified as either a regular miss or a block.  

<center>
$fgBlock_{g_k} \sim \mathcal Bernoulli(p_{block})$  
</center>  


$p_{block}$ is modeled using a GBM where the only predictors are yard line ^[Given a missed field goal, the shorter the attempt was, the more likely it was to be blocked] and down, where down likely only matters in the sense that it alerts the model as to whether the yard line might change between the current game state and any field goal try. The change in field position on regular field goal misses is modeled by a linear regression when the down is not fourth. On fourth down the ball is simply moved back 7 yards as is customary on missed field goals. Blocked field goals require the same protocol for determing change in field position as other possible outcomes. One linear model is used for downs first through third and another for fourth. For a given output $yardsMoved_{g_k}$ from the model $m$ specified by $outcome_{g_k}$ we take a sample:

<center>
$yardsDraw_{g_k} \sim \mathcal N(yardsMoved_{g_k}, \sigma^{2}_{\epsilon_m})$
</center>  


Samples drawn are rejected if they result in an invalid or illogical game state. Examples include first down conversions not by penalty where the draw would cause the game state to be short of the first down marker, a turnover on downs where the draw causes the ball to move past the first down marker, or any draw where the resulting game state features a yard line outside $[1, 99]$. 

Once we have samples for the number of yards the ball has moved downfield we can fully update each of the game states for which a scoring outcome was not sampled. Each of these remaining game states $g_k$ will then go through the process outlined in 2.2. This will lead to the creation of $N$ new game states advanced from each game state $g_k$ and will result in: 

$WinProb_{g_k} = \frac{1}{N}\sum_{j=1}^NWinProb_{g_{k_j}}$ where  
$WinProb_{g_{k_j}}$ is:  
  $CDF^{-1}(\frac{\mu_{g_{k_j}} + s_{curr_{g_{k_j}}}}{\sigma_{g_{k_j}}})$ for $t_{g_{k_j}} > 600$,   
  $GBM_{4th}({g_{k_j}})$ for $t_{g_{k_j}}$ such that $0 < t_{g_k} \leq 600$,   
  $GBM_{ot}({g_{k_j}})$ for $t_{g_{k_j}} < 0$  
  
The resulting $WinProb_i$ for the initial state $i$ is then computed by taking averaging the values for of win probability for each game state ${g_k}$^[This includes the game states for which $score{g_k}$ was equal to one and $WinProb{g_k}$ was calculated earlier.]: 

<center>
$WinProb_i = \frac{1}{n} \sum_{k=1}^nWinProb_{g_k}$
</center>

## Reasoning Behind Modeling Methodology

This methodology has several benefits. The first is that the impact of some game state descriptors like down, yards to go for a first down, and field position should theoretically be easier to measure on events they directly impact, like the outcome of a given set of downs, than on events they indirectly impact, like the probability of winning the game. Reducing noise around the measurement of the effects of game state descriptors like down, yards to go and yard line should improve model accuracy. Another benefit of modeling win probability like this is that it allows for a full distribution of EPA to be specified rather than a point estimate. This gives a better estimate of the variance associated with the game state which should lead to more accurate model predictions. Additionally, giving the model an estimate of the distribution of time that will elapse between the current game state and the next score gives the model an even fuller picture of the game and should help aid model accuracy, particularly towards the end of game. The modeling methodology also makes it easier to update the model piecemeal. If research comes out about the factors that influence the likelihood of converting a given set of downs, it shouldn’t be very hard to incorporate the findings into the multinomial models that are currently in place to handle predicting the outcome of a given set of downs. It’s also nice to get many of the benefits of a simulation without having to specify an exhaustive grid of conditional probabilities^[Though this modeling protocol certainly comes with its fair share of edge cases to account for.].
